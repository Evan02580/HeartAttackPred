import numpy as np
from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, balanced_accuracy_score
from loadData import read_data_all
from cluster import apply_clustering
from cluster import split_by_cluster
from sklearn.ensemble import RandomForestClassifier
import pandas as pd

def write_metrics_to_csv(metrics, filename, model_name="Logistic Regression"):
    filename = f"./results/{filename}.csv"
    df = pd.DataFrame([metrics])
    # æœ€å‰é¢åŠ ä¸€åˆ—
    df.insert(0, 'Dataset', model_name)
    df.to_csv(filename, mode='a', header=not pd.io.common.file_exists(filename), index=False)


if __name__ == "__main__":
    file_num = 0

    file_path = "./datasets/"
    file_name = ["heart", "heart-1", "UCI-1190-11", "statlog_heart"][file_num]
    label_col = ["HeartDisease", "target", "target", "target"][file_num]
    K_select = [[4, 6], [3, 5], [3, 4], [2, 3]][file_num]  # æ¯ä¸ªæ•°æ®é›†é€‰æ‹©çš„Kå€¼

    # Step 1: è¯»å–æ‰€æœ‰æ•°æ®
    X_all, y_all, feature_names = read_data_all(f"{file_path}{file_name}.csv", label_col=label_col)
    X_all = np.asarray(X_all)
    y_all = np.asarray(y_all)


    for best_k in K_select:
        print(f"\n===== Random Forest (Cluster = {best_k}) =====")
        model, cluster_labels = apply_clustering(X_all, best_k)
        split_data = split_by_cluster(X_all, y_all, cluster_labels)

        total_test_metrics = {'y': [], 'y_pred': []}
        for c, data in split_data.items():
            print(f"\n--- Cluster {c} ---")

            X_train = np.asarray(data["X_train"])
            y_train = np.asarray(data["y_train"])
            # X_val = data["X_val"]
            # y_val = data["y_val"]
            X_test = data["X_test"]
            y_test = data["y_test"]
            print(f"Train samples: {len(X_train)}, "
                  # f"Valid samples: {len(X_val)}, "
                  f"Test samples: {len(X_test)}, ", end='')
            print(f"Risk in Train: {sum(y_train) / len(y_train):.2f}, "
                  # f"{sum(y_val) / len(y_val):.2f}, "
                  f"Risk in Test: {sum(y_test) / len(y_test):.2f}")

            best_f1 = 0
            beat_acc = 0
            best_n = 0
            best_d = 0
            for n in range(2, 15):
                for d in range(2, 15):
                    rf = RandomForestClassifier(n_estimators=n, max_depth=d, random_state=42)
                    rf.fit(X_train, y_train)

                    y_pred = rf.predict(X_test)
                    y_prob = rf.predict_proba(X_test)[:, 1]
                    y = y_test
                    if f1_score(y, y_pred) > best_f1 or accuracy_score(y, y_pred) > beat_acc:
                        print(f"Dep: {d}, Est: {n}, "
                              f"F1: {f1_score(y, y_pred):.4f}, "
                              f"Acc: {accuracy_score(y, y_pred):.4f}, "
                              f"BalAcc: {balanced_accuracy_score(y, y_pred):.4f}, "
                              f"AUC: {roc_auc_score(y, y_prob):.4f}")
                        best_f1 = f1_score(y, y_pred)
                        beat_acc = accuracy_score(y, y_pred)
                        best_n = n
                        best_d = d

            print(f"depth: {best_d}, n_estimators: {best_n}")
            rf = RandomForestClassifier(n_estimators=best_n, max_depth=best_d, random_state=42)
            rf.fit(X_train, y_train)

            import os

            # åˆ›å»ºä¿å­˜å›¾ç‰‡çš„æ–‡ä»¶å¤¹
            output_dir = f"interpret/{file_name}/SHAP"
            os.makedirs(output_dir, exist_ok=True)

            # ========== SHAP è§£é‡Šï¼ˆä¿å­˜ summary plot ä¸ºå›¾ç‰‡ï¼‰ ==========
            import shap
            import matplotlib.pyplot as plt

            explainer = shap.TreeExplainer(rf)
            shap_values = explainer.shap_values(X_test)

            print(f"ç»˜åˆ¶å¹¶ä¿å­˜ Cluster {c} çš„ SHAP summary plot")
            shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)

            plt.title(f"Cluster {c} SHAP Summary Plot", fontsize=14)
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, f"shap_cluster{c}.png"))
            plt.close()

            # ========== LIME è§£é‡Šï¼ˆåªè§£é‡Š1ä¸ªæ ·æœ¬ï¼‰ ==========
            import lime.lime_tabular

            explainer_lime = lime.lime_tabular.LimeTabularExplainer(X_train, mode="classification")
            exp = explainer_lime.explain_instance(X_test[0], rf.predict_proba, num_features=10)
            print(f"è§£é‡Š Cluster {c} ä¸­ç¬¬ä¸€ä¸ªæµ‹è¯•æ ·æœ¬çš„ LIME è´¡çŒ®åº¦")
            exp.show_in_notebook(show_table=True)

            # ========== å•ä¸ªæ ·æœ¬ waterfall plot ==========
            sample_idx = 0
            sample = X_test[sample_idx].reshape(1, -1)
            pred_prob = rf.predict_proba(sample)
            print(f"Cluster {c} ä¸­ç¬¬ä¸€ä¸ªæ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡ï¼ˆä½é£é™©, é«˜é£é™©ï¼‰: {pred_prob}")


            shap.plots._waterfall.waterfall_legacy(
                explainer.expected_value[1],
                shap_values[1][sample_idx],
                feature_names=feature_names,
                show=False
            )
            plt.gcf().set_size_inches(10, 8)
            plt.subplots_adjust(top=0.9, bottom=0.1)
            plt.title(f"Cluster {c} Sample {sample_idx} SHAP Waterfall Plot", fontsize=14)
            plt.savefig(os.path.join(output_dir, f"shap_waterfall_cluster{c}.png"))
            plt.close()

            for name, X, y in [("Train", X_train, y_train), (" Test", X_test, y_test)]:
                y_pred = rf.predict(X)
                y_prob = rf.predict_proba(X)[:, 1]
                metrics = {"F1": f1_score(y, y_pred),
                           "Accuracy": accuracy_score(y, y_pred),
                           "Balanced Accuracy": balanced_accuracy_score(y, y_pred),
                           "AUC": roc_auc_score(y, y_prob),
                           "Samples": len(X)}
                print(
                    f"{name} - F1: {metrics['F1']:.4f}, "
                    f"Acc: {metrics['Accuracy']:.4f}, "
                    f"BalAcc: {metrics['Balanced Accuracy']:.4f}, "
                    f"AUC: {metrics['AUC']:.4f}")
                if name == " Test":
                    total_test_metrics['y'].extend(y)
                    total_test_metrics['y_pred'].extend(y_pred)
        # åŠ æƒå¹³å‡
        total_y = total_test_metrics['y']
        total_y_pred = total_test_metrics['y_pred']

        # åˆå§‹åŒ–åŠ æƒæŒ‡æ ‡
        weighted_avg = {
            "F1": round(f1_score(total_y, total_y_pred), 4),
            "Accuracy": round(accuracy_score(total_y, total_y_pred), 4),
            "Balanced Accuracy": round(balanced_accuracy_score(total_y, total_y_pred), 4),
            "AUC": round(roc_auc_score(total_y, total_y_pred), 4)
        }

        # æ‰“å°åŠ æƒå¹³å‡ç»“æœ
        print(f"\n===== Random Forest (Cluster = {best_k}) =====")
        print(f"Data Name: {file_name}")
        print(f"Total Samples: {len(y_all)}")
        for k, v in weighted_avg.items():
            print(f"{k}: {v}")
        # ä¿å­˜ç»“æœåˆ° CSV
        # write_metrics_to_csv(weighted_avg, file_name, model_name=f"CluRF (k = {best_k})")



"""
if __name__ == "__main__":
    file_path = "./datasets/heart-attack-risk-prediction-dataset.csv"

    # Step 1: è¯»å–æ‰€æœ‰æ•°æ®
    X_all, y_all = read_data_all(file_path)

    # Step 2: å…¨æ•°æ®èšç±»
    best_k = 4
    model, cluster_labels = apply_clustering(X_all, best_k) # æ•°æ®åˆ’åˆ†

    # Step 3: æŒ‰ cluster åˆ†åˆ«åˆ’åˆ†æ•°æ®
    split_data = split_by_cluster(X_all, y_all, cluster_labels)

    # Step 4: éå†æ¯ä¸ª cluster å¹¶è®­ç»ƒæ¨¡å‹RF
    n_estimators_list = [5]  # 100 å±•ç°å‡ºæ¯”è¾ƒå¥½çš„ ä½†0.6 - 0.68ä¹‹é—´
    for n_estimators in n_estimators_list:
        print(f"\n===== Random Forest (n_estimators={n_estimators}) =====")

        for c, data in split_data.items():
            print(f"\n--- Cluster {c} ---")
            X_train = data["X_train"]
            y_train = data["y_train"]
            X_val = data["X_val"]
            y_val = data["y_val"]
            X_test = data["X_test"]
            y_test = data["y_test"]

            X_train = np.asarray(X_train)
            y_train = np.asarray(y_train)
            try:
                smote = SMOTE(random_state=42)
                X_train, y_train = smote.fit_resample(X_train, y_train)

                count_0 = np.sum(y_train == 0)
                count_1 = np.sum(y_train == 1)
                ratio_1 = count_1 / (count_0 + count_1 + 1e-6)
                print(f"ğŸ“Š SMOTEå Cluster {c} æ ·æœ¬åˆ†å¸ƒï¼š0ç±»={count_0}, 1ç±»={count_1}, 1ç±»å æ¯”={ratio_1:.2%}")
            except ValueError as e:
                print(f"âš ï¸ SMOTEå¤±è´¥ï¼ŒCluster {c} æ•°æ®è¿‡å°‘: {e}")


            rf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
            xgb = XGBClassifier(n_estimators=n_estimators, use_label_encoder=False, eval_metric='logloss', random_state=42)
            # ç»„åˆ Voting æ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰
            ensemble = VotingClassifier(estimators=[
                ('rf', rf),
                ('xgb', xgb)
            ], voting='soft')
            # æ‹Ÿåˆæ¨¡å‹
            ensemble.fit(X_train, y_train)

            #clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
            #clf.fit(X_train, y_train)

            for name, X, y in [("Train", X_train, y_train), ("Val", X_val, y_val), ("Test", X_test, y_test)]:
                y_pred = ensemble.predict(X)
                y_prob = ensemble.predict_proba(X)[:, 1]
                print(
                    f"{name} - F1: {f1_score(y, y_pred):.4f}, "
                    f"Acc: {accuracy_score(y, y_pred):.4f}, "
                    f"BalAcc: {balanced_accuracy_score(y, y_pred):.4f}, "
                    f"AUC: {roc_auc_score(y, y_prob):.4f}")

            #  åŠ å…¥ SMOTE è¿‡é‡‡æ · è¿™ä¸ªpythonç‰ˆæœ¬ä¸è¡Œ å¾—3.7 æˆ‘ä¸èƒ½åŠ¨æˆ‘çš„é¡¹ç›®ç¯å¢ƒ æˆ‘å¾—è·‘cv
"""
