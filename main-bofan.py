import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, balanced_accuracy_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from loadData import read_data_all
from cluster import apply_clustering
from cluster import split_by_cluster
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from imblearn.over_sampling import SMOTE


if __name__ == "__main__":
    file_path = "./datasets/heart-attack-risk-prediction-dataset.csv"

    # Step 1: è¯»å–æ‰€æœ‰æ•°æ®
    X_all, y_all = read_data_all(file_path)
    X_all = np.asarray(X_all)
    y_all = np.asarray(y_all)

    # âœ… Step 2: å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œ SMOTE å¤„ç†ï¼ˆå…ˆå¹³è¡¡ç±»åˆ«ï¼‰
    try:
        smote = SMOTE(random_state=42)
        X_all, y_all = smote.fit_resample(X_all, y_all)
        count_0 = np.sum(y_all == 0)
        count_1 = np.sum(y_all == 1)
        ratio_1 = count_1 / (count_0 + count_1 + 1e-6)
        print(f"ğŸ“Š å…¨æ•°æ® SMOTEåæ ·æœ¬åˆ†å¸ƒï¼š0ç±»={count_0}, 1ç±»={count_1}, 1ç±»å æ¯”={ratio_1:.2%}")
    except ValueError as e:
        print(f"âš ï¸ å…¨æ•°æ® SMOTEå¤±è´¥: {e}")

    # âœ… Step 3: èšç±»å¤„ç†
    best_k = 4
    model, cluster_labels = apply_clustering(X_all, best_k)

    # âœ… Step 4: æŒ‰ cluster åˆ†åˆ«åˆ’åˆ†æ•°æ®ï¼ˆå†…éƒ¨ä¸å†åš SMOTEï¼‰
    split_data = split_by_cluster(X_all, y_all, cluster_labels)

    # Step 5: éå†æ¯ä¸ª cluster å¹¶è®­ç»ƒ Voting æ¨¡å‹
    n_estimators_list = [10]
    for n_estimators in n_estimators_list:
        print(f"\n===== Random Forest (n_estimators={n_estimators}) =====")
        total_test_metrics = []
        for c, data in split_data.items():
            print(f"\n--- Cluster {c} ---")

            X_train = np.asarray(data["X_train"])
            y_train = np.asarray(data["y_train"])
            X_val = data["X_val"]
            y_val = data["y_val"]
            X_test = data["X_test"]
            y_test = data["y_test"]
            print(f"Cluster {c} - "
                  f"Train samples: {len(X_train)}, "
                  f"Val samples: {len(X_val)}, "
                  f"Test samples: {len(X_test)}")

            # ä¸å†å¯¹æ¯ä¸ª cluster å†…éƒ¨åš SMOTE

            rf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
            xgb = XGBClassifier(n_estimators=n_estimators, use_label_encoder=False, eval_metric='logloss', random_state=42)
            ensemble = VotingClassifier(estimators=[('rf', rf), ('xgb', xgb)], voting='soft')
            ensemble.fit(X_train, y_train)

            for name, X, y in [("Train", X_train, y_train), ("Val", X_val, y_val), ("Test", X_test, y_test)]:
                y_pred = ensemble.predict(X)
                y_prob = ensemble.predict_proba(X)[:, 1]
                metrics = {"F1": f1_score(y, y_pred),
                           "Accuracy": accuracy_score(y, y_pred),
                           "Balanced Accuracy": balanced_accuracy_score(y, y_pred),
                           "AUC": roc_auc_score(y, y_prob),
                           "Samples": len(X)}
                print(
                    f"{name} - F1: {metrics["F1"]:.4f}, "
                    f"Acc: {metrics["Accuracy"]:.4f}, "
                    f"BalAcc: {metrics["Balanced Accuracy"]:.4f}, "
                    f"AUC: {metrics["AUC"]:.4f}")
                if name == "Test":
                    total_test_metrics.append(metrics)
        # åŠ æƒå¹³å‡



"""
if __name__ == "__main__":
    file_path = "./datasets/heart-attack-risk-prediction-dataset.csv"

    # Step 1: è¯»å–æ‰€æœ‰æ•°æ®
    X_all, y_all = read_data_all(file_path)

    # Step 2: å…¨æ•°æ®èšç±»
    best_k = 4
    model, cluster_labels = apply_clustering(X_all, best_k) # æ•°æ®åˆ’åˆ†

    # Step 3: æŒ‰ cluster åˆ†åˆ«åˆ’åˆ†æ•°æ®
    split_data = split_by_cluster(X_all, y_all, cluster_labels)

    # Step 4: éå†æ¯ä¸ª cluster å¹¶è®­ç»ƒæ¨¡å‹RF
    n_estimators_list = [5]  # 100 å±•ç°å‡ºæ¯”è¾ƒå¥½çš„ ä½†0.6 - 0.68ä¹‹é—´
    for n_estimators in n_estimators_list:
        print(f"\n===== Random Forest (n_estimators={n_estimators}) =====")

        for c, data in split_data.items():
            print(f"\n--- Cluster {c} ---")
            X_train = data["X_train"]
            y_train = data["y_train"]
            X_val = data["X_val"]
            y_val = data["y_val"]
            X_test = data["X_test"]
            y_test = data["y_test"]

            X_train = np.asarray(X_train)
            y_train = np.asarray(y_train)
            try:
                smote = SMOTE(random_state=42)
                X_train, y_train = smote.fit_resample(X_train, y_train)

                count_0 = np.sum(y_train == 0)
                count_1 = np.sum(y_train == 1)
                ratio_1 = count_1 / (count_0 + count_1 + 1e-6)
                print(f"ğŸ“Š SMOTEå Cluster {c} æ ·æœ¬åˆ†å¸ƒï¼š0ç±»={count_0}, 1ç±»={count_1}, 1ç±»å æ¯”={ratio_1:.2%}")
            except ValueError as e:
                print(f"âš ï¸ SMOTEå¤±è´¥ï¼ŒCluster {c} æ•°æ®è¿‡å°‘: {e}")


            rf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
            xgb = XGBClassifier(n_estimators=n_estimators, use_label_encoder=False, eval_metric='logloss', random_state=42)
            # ç»„åˆ Voting æ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰
            ensemble = VotingClassifier(estimators=[
                ('rf', rf),
                ('xgb', xgb)
            ], voting='soft')
            # æ‹Ÿåˆæ¨¡å‹
            ensemble.fit(X_train, y_train)

            #clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
            #clf.fit(X_train, y_train)

            for name, X, y in [("Train", X_train, y_train), ("Val", X_val, y_val), ("Test", X_test, y_test)]:
                y_pred = ensemble.predict(X)
                y_prob = ensemble.predict_proba(X)[:, 1]
                print(
                    f"{name} - F1: {f1_score(y, y_pred):.4f}, "
                    f"Acc: {accuracy_score(y, y_pred):.4f}, "
                    f"BalAcc: {balanced_accuracy_score(y, y_pred):.4f}, "
                    f"AUC: {roc_auc_score(y, y_prob):.4f}")

            #  åŠ å…¥ SMOTE è¿‡é‡‡æ · è¿™ä¸ªpythonç‰ˆæœ¬ä¸è¡Œ å¾—3.7 æˆ‘ä¸èƒ½åŠ¨æˆ‘çš„é¡¹ç›®ç¯å¢ƒ æˆ‘å¾—è·‘cv
"""
